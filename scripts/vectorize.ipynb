{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['configuration.properties']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PyPDF2 import PdfReader\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from pinecone import Pinecone\n",
    "from litellm import completion\n",
    "import litellm\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('configuration.properties')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinecone_connection():\n",
    "    try:\n",
    "        pinecone_api_key = config['PINECONE']['pinecone_api_key']\n",
    "        index_name = config['PINECONE']['index']\n",
    "        return pinecone_api_key, index_name\n",
    "    except Exception as e:\n",
    "        print(\"Exception in pinecone_connection function: \",e)\n",
    "        return \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_data(pdf_link):\n",
    "    try:\n",
    "        response = requests.get(pdf_link)\n",
    "        if response.status_code == 200:\n",
    "            pdf_bytes = BytesIO(response.content)\n",
    "\n",
    "            # Step 2: Read and extract text\n",
    "            reader = PdfReader(pdf_bytes)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        else:\n",
    "            print(\"Failed to download the PDF.\")\n",
    "            text = \"\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(\"Exception in read_pdf_data function: \",e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=512):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        tokens = tokenizer.encode(text, truncation=False, add_special_tokens=False)\n",
    "\n",
    "        # Split tokens into chunks of max `chunk_size`\n",
    "        token_chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "        # Decode token chunks back to text\n",
    "        text_chunks = [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in token_chunks]\n",
    "\n",
    "        return text_chunks\n",
    "    except Exception as e:\n",
    "        print(\"Exception in chunk_text function:\", e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(prompt, text):\n",
    "    try:\n",
    "        # Log API base URL\n",
    "        api_base = config['OLLAMA']['OLLAMA_API_BASE']\n",
    "\n",
    "        # Test connection first\n",
    "        conn_test = requests.get(f\"{api_base}/api/version\")\n",
    "\n",
    "        # Hardcoded test query\n",
    "        test_message = f\"{prompt} {text}\"\n",
    "\n",
    "        # Make request to Ollama with more detailed logging\n",
    "        response = completion(\n",
    "            model=\"ollama/llama3.2:1b\",  # Updated model name to match available model\n",
    "            messages=[{\"role\": \"user\", \"content\": test_message}],\n",
    "            api_base=api_base,\n",
    "            temperature=0.7,\n",
    "            request_timeout=30\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"Exception in summarize_text function: \",e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def embedding(final_summary):\n",
    "    try:\n",
    "\n",
    "        response = litellm.embedding(\n",
    "            model=\"ollama/nomic-embed-text\",  # Ollama model\n",
    "            # Log API base URL\n",
    "            api_base = config['OLLAMA']['OLLAMA_API_BASE'], # Ollama API base\n",
    "            input=final_summary\n",
    "        )\n",
    "\n",
    "        return response.data[0][\"embedding\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception in embedding function: \",e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_to_pinecone(embedded_vector):\n",
    "    pinecone_api_key, index_name = pinecone_connection()\n",
    "    print(\"Pinecone API Key: \", pinecone_api_key)\n",
    "    print(\"Index Name: \", index_name)\n",
    "    if pinecone_api_key == \"\" or index_name == \"\":\n",
    "        return None\n",
    "\n",
    "    pinecone = Pinecone(api_key=pinecone_api_key)\n",
    "    index = pinecone.Index(name=index_name)\n",
    "\n",
    "from embed.connections import snowflake_connection, pinecone_connection, aws_connection\n",
    "from pinecone import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from io import BytesIO, StringIO\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fetch_table_from_s3(csv_key):\n",
    "    '''\n",
    "    function to fetch latest(past 24 hours) jobs data from S3\n",
    "    '''\n",
    "    try:\n",
    "        s3_client, bucket_name = aws_connection()\n",
    "        print(\"AWS connection established. Fetching data\")\n",
    "\n",
    "        # fetching raw csv\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"validated_jobs/{csv_key}\")\n",
    "        csv_obj_content = response['Body'].read()\n",
    "        \n",
    "        # file like object creation\n",
    "        pdfFileObj = BytesIO(csv_obj_content)\n",
    "        pdf_df = pd.read_csv(pdfFileObj, sep=\"|\")\n",
    "        pdf_df = pdf_df.replace(np.nan, None)\n",
    "        print(\"CSV fetched. Length of data: \", len(pdf_df))\n",
    "\n",
    "        return pdf_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception in fetch_data_from_snowflake function\", e)\n",
    "        return ''\n",
    "\n",
    "\n",
    "def storing_pinecone(csv_key):\n",
    "    '''\n",
    "    function to store set a in pinecone\n",
    "    '''\n",
    "    try:\n",
    "        df = fetch_table_from_s3(csv_key)\n",
    "        df.drop(columns=['min_salary', 'max_salary', 'employment_type', 'source'], inplace=True)\n",
    "        print(df.head())\n",
    "        \n",
    "        # Pinecone\n",
    "        pinecone_api_key, index_name = pinecone_connection()\n",
    "        pinecone = Pinecone(api_key=pinecone_api_key)\n",
    "        index = pinecone.Index(name=index_name)\n",
    "\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        if device != 'cuda':\n",
    "            model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "        all_embeddings = []\n",
    "\n",
    "        # iterating over pandas dataframe\n",
    "        print(\"Generating embeddings..\")\n",
    "        for _, row in df.iterrows():\n",
    "            _id = str(row['job_id'])\n",
    "            job_title = row['job_title']\n",
    "            company = row['company']\n",
    "            location = row['job_location']\n",
    "            url = row['job_url']\n",
    "            date_posted = row['date_posted']\n",
    "            description = row['job_desc']\n",
    "\n",
    "            # Concatenating all relevant text data for embedding with new lines\n",
    "            full_text = f\"{job_title}\\n{company}\\n{location}\\n{url}\\n{date_posted}\\n{description}\"\n",
    "            display_text = f\"job_title: {job_title}\\ncompany: {company}\\nlocation: {location}\\nurl: {url}\\ndate_posted: {date_posted}\"\n",
    "\n",
    "            # embedding data\n",
    "            embedding = model.encode(full_text)\n",
    "\n",
    "            print(f\"Embedded job for id: {_id}\")\n",
    "\n",
    "            embedding_data = {\n",
    "                \"id\": _id,\n",
    "                \"values\": embedding,\n",
    "                \"metadata\": {\n",
    "                    \"id\": _id,\n",
    "                    \"file_name\": 'jobmatch_data.csv',\n",
    "                    \"text\": display_text\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # embedding question and answer separately\n",
    "            all_embeddings.append(embedding_data)\n",
    "        print(len(embedding))\n",
    "        print(\"Embeddings generated\")\n",
    "\n",
    "        # upserting the embeddings to pinecone namespace\n",
    "        index.upsert(all_embeddings)\n",
    "\n",
    "        return \"successful\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception in storing_pinecone() function: \", e)\n",
    "        return \"failed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_link = 'https://public-inspection.federalregister.gov/2025-00479.pdf?1736516733'\n",
    "text = read_pdf_data(pdf_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_text(text, chunk_size=512)\n",
    "summarized_text_chunk = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Final Summary:\n",
      "There are several important updates to reimbursement rates for the Summer Food Service Program (SFSP), which provides financial assistance to food service providers who serve children in low-income families. Here's a summary of the key changes:\n",
      "\n",
      "**Reimbursement Rate Increases**\n",
      "\n",
      "* The annual increases will take effect starting January 1, 2024.\n",
      "* For meal periods that are combined and individually authorized by the Food Away from Home series of the Consumer Price Index (CPI), the rate adjustments will be applied to each component separately.\n",
      "\n",
      "**Breakdown of Reimbursement Rates for SFSP**\n",
      "\n",
      "| Meal Period | Reimbursement Rate |\n",
      "| --- | --- |\n",
      "| Breakfast | $3.09-$3.03 per meal, or $8.08-$8.61 per meal (up from $2.78-$2.83) |\n",
      "| Lunch/Supper | $5.00-$5.31 per meal, or $8.76-$8.77 per meal (up from $4.50-$4.63) |\n",
      "| Snack | $1.28-$1.25 per snack, or $2.02-$2.06 per snack (no change) |\n",
      "\n",
      "**Reimbursement Rate Changes for SFSP Providers**\n",
      "\n",
      "* Alaska: Rural or self-prep sites:\n",
      "\t+ Breakfast: +$0.55\n",
      "\t+ Lunch/Supper: +$3.00\n",
      "\t+ Snack: +$0.75\n",
      "* Guam, Hawaii, Puerto Rico, and U.S. Virgin Islands: Rural or self-prep sites:\n",
      "\t+ Breakfast: +$1.25\n",
      "\t+ Lunch/Supper: +$2.50\n",
      "\t+ Snack: +$0.50\n",
      "\n",
      "**Important Notes**\n",
      "\n",
      "* These changes apply to all other types of sites except Alaska (rural or self-prep) and Hawaii (except rural or self-prep).\n",
      "* The reimbursement rates are based on the Food Away from Home series of the Consumer Price Index (CPI), which takes into account inflation.\n",
      "* For meal periods that are combined and individually authorized by the CPI, the rate adjustments will be applied to each component separately.\n",
      "\n",
      "It's essential for food service providers who participate in the SFSP to review these changes carefully and adjust their operations accordingly to ensure compliance with the updated reimbursement rates.\n"
     ]
    }
   ],
   "source": [
    "chunk_summzrize_prompt = \"Below is the pdf content of the regulation update by goverment agencies in food and agriculture industry. Can you summarize the important content that can affect the small scale food businesses? In summary, keep the important content like agency name and numbers and remove the unnecessary details.\"\n",
    "final_summary_prompt = \"Can you strictly summarize the important content from the below summarized text? Keep important content like agency name and numbers and remove the unnecessary details.\"\n",
    "\n",
    "for _,text in enumerate(chunks):\n",
    "    summarized_text_chunk += summarize_text(chunk_summzrize_prompt, text)\n",
    "\n",
    "print(\"-----------------\")\n",
    "print(\"Final Summary:\")\n",
    "final_summary = summarize_text(final_summary_prompt, summarized_text_chunk)\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in embedding function:  litellm.APIConnectionError: asyncio.run() cannot be called from a running event loop\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Old Laptop\\Hackathon Dev\\Civic Hackathon\\civic_ai\\venv\\lib\\site-packages\\litellm\\main.py\", line 3703, in embedding\n",
      "    response = ollama_embeddings_fn(  # type: ignore\n",
      "  File \"d:\\Old Laptop\\Hackathon Dev\\Civic Hackathon\\civic_ai\\venv\\lib\\site-packages\\litellm\\llms\\ollama\\completion\\handler.py\", line 92, in ollama_embeddings\n",
      "    return asyncio.run(\n",
      "  File \"C:\\Users\\raisi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\runners.py\", line 33, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: asyncio.run() cannot be called from a running event loop\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedded_vector = await embedding(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
